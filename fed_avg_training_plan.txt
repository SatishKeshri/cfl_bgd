Firstly train 5 client models for some number of epochs(i.e. some number of iterations)
this point would mark the end of a round.Here,we combine the 5 models
and repeat this thing till all the iterations are complete.

Important things to be kept in mind
-Where to mark(the iteration number of round 1 end) the end of a round.
-Combine these 5 models to get a single model.
-Propogate this agg. model to all the clients and continue training
from the next iteration.

-Every client model will have its own optimizer,own criterion object and others.


After an iteration the model's param,mean_param,std_param get updated.(param will be equal to mean_param)


#The model trained parameters,mean,std can be accessed using trainer.optimizer.param_groups this is a list of dictionaries(layers)
where each layer's dict has keys params,mean_param,std_param etc.




Points to check next

-Check if the the sampler in the train_loader is at correct iteration after every round
-Check if the aggregated model is correctly reflected at each client in the next round

-Before round 1 also,initialize model once a give its copy to all the clients


#Fully working command

python main.py --logname continuous_permuted_mnist_10_tasks --num_workers 1 --permute_seed 2019 --seed 1000 --iterations_per_virtual_epc 469 --contpermuted_beta 4 --num_of_permutations $(( 3 - 1 )) --optimizer bgd --nn_arch mnist_simple_net_200width_domainlearning_784input_10cls_1ds --dataset ds_cont_permuted_mnist --num_epochs $(( 3 * 3)) --std_init 0.06 --batch_size 128 --results_dir perm_mnist_10_tasks_100_epochs --train_mc_iters 10 --inference_map --federated_learning --n_clients 5 --mean_eta 0.001



-Next steps to avoid nan loss
-Try batch normalisation
-Try leaky relu activation function


# bgd perfectly working

CUDA_VISIBLE_DEVICES=1 python main.py --num_workers 1 --permute_seed 2019 --seed 1000 --iterations_per_virtual_epc 469 --contpermuted_beta 4 --num_of_permutations $(( 10 - 1 )) --optimizer bgd --nn_arch mnist_simple_net_200width_domainlearning_784input_10cls_1ds --dataset ds_cont_permuted_mnist --num_epochs $(( 10 * 10)) --std_init 0.06 --batch_size 128 --train_mc_iters 10 --inference_map --federated_learning --n_clients 5 --mean_eta 0.01 


# sgd 

CUDA_VISIBLE_DEVICES=0 python main.py --num_workers 1 --permute_seed 2019 --seed 1000 --iterations_per_virtual_epc 469 --contpermuted_beta 4 --num_of_permutations $(( 10 - 1 )) --optimizer sgd --nn_arch mnist_simple_net_200width_domainlearning_784input_10cls_1ds --dataset ds_cont_permuted_mnist --num_epochs $(( 10 * 10)) --batch_size 128 --train_mc_iters 1 --inference_map --federated_learning --n_clients 5 --lr 0.01




#Centralised setting commands



# bgd perfectly working

CUDA_VISIBLE_DEVICES=1 python main.py --num_workers 1 --permute_seed 2019 --seed 1000 --iterations_per_virtual_epc 469 --contpermuted_beta 4 --num_of_permutations $(( 10 - 1 )) --optimizer bgd --nn_arch mnist_simple_net_200width_domainlearning_784input_10cls_1ds --dataset ds_cont_permuted_mnist --num_epochs $(( 10 * 10)) --std_init 0.06 --batch_size 128 --train_mc_iters 10 --inference_map --mean_eta 0.01 


# sgd 

CUDA_VISIBLE_DEVICES=0 python main.py --num_workers 1 --permute_seed 2019 --seed 1000 --iterations_per_virtual_epc 469 --contpermuted_beta 4 --num_of_permutations $(( 10 - 1 )) --optimizer sgd --nn_arch mnist_simple_net_200width_domainlearning_784input_10cls_1ds --dataset ds_cont_permuted_mnist --num_epochs $(( 10 * 10)) --batch_size 128 --train_mc_iters 1 --inference_map --lr 0.01


# federated addition -> increasing rounds

# bgd, beta = 4

CUDA_VISIBLE_DEVICES=1 python main.py --num_workers 1 --permute_seed 2019 --seed 1000 --iterations_per_virtual_epc 469 --contpermuted_beta 4 --num_of_permutations $(( 10 - 1 )) --optimizer bgd --nn_arch mnist_simple_net_200width_domainlearning_784input_10cls_1ds --dataset ds_cont_permuted_mnist --num_epochs $(( 10 * 10)) --std_init 0.06 --batch_size 128 --train_mc_iters 10 --inference_map --federated_learning --n_clients 5 --mean_eta 1 --num_aggs_per_task 5


# sgd, beta = 4

CUDA_VISIBLE_DEVICES=0 python main.py --num_workers 1 --permute_seed 2019 --seed 1000 --iterations_per_virtual_epc 469 --contpermuted_beta 4 --num_of_permutations $(( 10 - 1 )) --optimizer sgd --nn_arch mnist_simple_net_200width_domainlearning_784input_10cls_1ds --dataset ds_cont_permuted_mnist --num_epochs $(( 10 * 10)) --batch_size 128 --train_mc_iters 1 --inference_map --federated_learning --n_clients 5 --lr 0.01 --num_aggs_per_task 5 



# modified aggregation with varying "max_grad_norm" [bgd 10 rounds with grad clip]

# max_grad_norm = 0.5, grad_clip

CUDA_VISIBLE_DEVICES=1 python main.py --num_workers 1 --permute_seed 2019 --seed 1000 --iterations_per_virtual_epc 469 --contpermuted_beta 4 --num_of_permutations $(( 10 - 1 )) --optimizer bgd --nn_arch mnist_simple_net_200width_domainlearning_784input_10cls_1ds --dataset ds_cont_permuted_mnist --num_epochs $(( 10 * 10)) --std_init 0.06 --batch_size 128 --train_mc_iters 10 --inference_map --federated_learning --n_clients 5 --mean_eta 1 --grad_clip --max_grad_norm 0.5

# same with max_grad_norm = 1.5, 2



# experiments to schedule [bgd_n_sgd : new_aggregation] [10 rounds]

python main.py --num_workers 1 --permute_seed 2019 --seed 1000 --iterations_per_virtual_epc 469 --contpermuted_beta 4 --num_of_permutations $(( 10 - 1 )) --optimizer bgd --nn_arch mnist_simple_net_200width_domainlearning_784input_10cls_1ds --dataset ds_cont_permuted_mnist --num_epochs $(( 10 * 10)) --std_init 0.06 --batch_size 128 --train_mc_iters 10 --inference_map --federated_learning --n_clients 5 --mean_eta 1 --grad_clip --max_grad_norm 

python main.py --num_workers 1 --permute_seed 2019 --seed 1000 --iterations_per_virtual_epc 469 --contpermuted_beta 4 --num_of_permutations $(( 10 - 1 )) --optimizer sgd --nn_arch mnist_simple_net_200width_domainlearning_784input_10cls_1ds --dataset ds_cont_permuted_mnist --num_epochs $(( 10 * 10)) --batch_size 128 --train_mc_iters 1 --inference_map --federated_learning --n_clients 5 --lr 0.01 



# Split mnist with bgd and sgd 

| iid

python main.py --num_workers 1 --permute_seed 2019 --seed 1000 --iterations_per_virtual_epc 469 --contpermuted_beta 4 --optimizer bgd --nn_arch mnist_simple_net_200width_domainlearning_784input_2cls_1ds --dataset ds_cont_split_mnist --num_epochs $(( 5 * 10)) --std_init 0.06 --batch_size 128 --train_mc_iters 10 --inference_map --federated_learning --n_clients 5 --mean_eta 1 --grad_clip --max_grad_norm 1.0

python main.py --num_workers 1 --permute_seed 2019 --seed 1000 --iterations_per_virtual_epc 469 --contpermuted_beta 4 --optimizer sgd --nn_arch mnist_simple_net_200width_domainlearning_784input_2cls_1ds --dataset ds_cont_split_mnist --num_epochs $(( 5 * 10)) --batch_size 128 --train_mc_iters 1 --inference_map --federated_learning --n_clients 5 --lr 0.01 


| non-iid

python main.py --num_workers 1 --permute_seed 2019 --seed 1000 --iterations_per_virtual_epc 469 --contpermuted_beta 4 --optimizer bgd --nn_arch mnist_simple_net_200width_domainlearning_784input_2cls_1ds --dataset ds_cont_split_mnist --num_epochs $(( 5 * 10)) --std_init 0.06 --batch_size 128 --train_mc_iters 10 --inference_map --federated_learning --n_clients 5 --mean_eta 1 --grad_clip --max_grad_norm 1.0 --non_iid_split

python main.py --num_workers 1 --permute_seed 2019 --seed 1000 --iterations_per_virtual_epc 469 --contpermuted_beta 4 --optimizer sgd --nn_arch mnist_simple_net_200width_domainlearning_784input_2cls_1ds --dataset ds_cont_split_mnist --num_epochs $(( 5 * 10)) --batch_size 128 --train_mc_iters 1 --inference_map --federated_learning --n_clients 5 --lr 0.01 --non_iid_split 




# Permuted mnist with 5 Tasks [5 rounds] 

| iid 

python main.py --num_workers 1 --permute_seed 2019 --seed 1000 --iterations_per_virtual_epc 469 --contpermuted_beta 4 --num_of_permutations $(( 5 - 1 )) --optimizer bgd --nn_arch mnist_simple_net_200width_domainlearning_784input_10cls_1ds --dataset ds_cont_permuted_mnist --num_epochs $(( 5 * 10)) --std_init 0.06 --batch_size 128 --train_mc_iters 10 --inference_map --federated_learning --n_clients 5 --mean_eta 1 --grad_clip --max_grad_norm 1  

python main.py --num_workers 1 --permute_seed 2019 --seed 1000 --iterations_per_virtual_epc 469 --contpermuted_beta 4 --num_of_permutations $(( 5 - 1 )) --optimizer sgd --nn_arch mnist_simple_net_200width_domainlearning_784input_10cls_1ds --dataset ds_cont_permuted_mnist --num_epochs $(( 5 * 10)) --batch_size 128 --train_mc_iters 1 --inference_map --federated_learning --n_clients 5 --lr 0.01 

| Non-iid

python main.py --num_workers 1 --permute_seed 2019 --seed 1000 --iterations_per_virtual_epc 469 --contpermuted_beta 4 --num_of_permutations $(( 5 - 1 )) --optimizer bgd --nn_arch mnist_simple_net_200width_domainlearning_784input_10cls_1ds --dataset ds_cont_permuted_mnist --num_epochs $(( 5 * 10)) --std_init 0.06 --batch_size 128 --train_mc_iters 10 --inference_map --federated_learning --n_clients 5 --mean_eta 1 --grad_clip --max_grad_norm 1  --non_iid

python main.py --num_workers 1 --permute_seed 2019 --seed 1000 --iterations_per_virtual_epc 469 --contpermuted_beta 4 --num_of_permutations $(( 5 - 1 )) --optimizer sgd --nn_arch mnist_simple_net_200width_domainlearning_784input_10cls_1ds --dataset ds_cont_permuted_mnist --num_epochs $(( 5 * 10)) --batch_size 128 --train_mc_iters 1 --inference_map --federated_learning --n_clients 5 --lr 0.01 --non_iid 



# Vanilla bgd n sgd

python main.py --num_workers 1 --permute_seed 2019 --seed 1000 --iterations_per_virtual_epc 469 --contpermuted_beta 4 --num_of_permutations $(( 5 - 1 )) --optimizer bgd --nn_arch mnist_simple_net_200width_domainlearning_784input_10cls_1ds --dataset ds_cont_permuted_mnist --num_epochs $(( 5 * 10)) --std_init 0.06 --batch_size 128 --train_mc_iters 10 --inference_map --mean_eta 1 

python main.py --num_workers 1 --permute_seed 2019 --seed 1000 --iterations_per_virtual_epc 469 --contpermuted_beta 4 --num_of_permutations $(( 5 - 1 )) --optimizer sgd --nn_arch mnist_simple_net_200width_domainlearning_784input_10cls_1ds --dataset ds_cont_permuted_mnist --num_epochs $(( 5 * 10)) --batch_size 128 --train_mc_iters 1 --inference_map --lr 0.01 


# Dirichlet [perm_5_tasks_non_iid]

python main.py --num_workers 1 --permute_seed 2019 --seed 1000 --iterations_per_virtual_epc 469 --contpermuted_beta 4 --num_of_permutations $(( 5 - 1 )) --optimizer bgd --nn_arch mnist_simple_net_200width_domainlearning_784input_10cls_1ds --dataset ds_cont_permuted_mnist --num_epochs $(( 5 * 10)) --std_init 0.06 --batch_size 128 --train_mc_iters 10 --inference_map --federated_learning --n_clients 5 --mean_eta 1 --grad_clip --max_grad_norm 1  --non_iid --alpha 0.01

python main.py --num_workers 1 --permute_seed 2019 --seed 1000 --iterations_per_virtual_epc 469 --contpermuted_beta 4 --num_of_permutations $(( 5 - 1 )) --optimizer sgd --nn_arch mnist_simple_net_200width_domainlearning_784input_10cls_1ds --dataset ds_cont_permuted_mnist --num_epochs $(( 5 * 10)) --batch_size 128 --train_mc_iters 1 --inference_map --federated_learning --n_clients 5 --lr 0.01 --non_iid --alpha 0.01 


# Split mnist dirchilet

python main.py --num_workers 1 --permute_seed 2019 --seed 1000 --iterations_per_virtual_epc 469 --contpermuted_beta 4 --optimizer bgd --nn_arch mnist_simple_net_200width_domainlearning_784input_2cls_1ds --dataset ds_cont_split_mnist --num_epochs $(( 5 * 10)) --std_init 0.06 --batch_size 128 --train_mc_iters 10 --inference_map --federated_learning --n_clients 5 --mean_eta 1 --grad_clip --max_grad_norm 1.0 --non_iid_split --alpha 1

python main.py --num_workers 1 --permute_seed 2019 --seed 1000 --iterations_per_virtual_epc 469 --contpermuted_beta 4 --optimizer sgd --nn_arch mnist_simple_net_200width_domainlearning_784input_2cls_1ds --dataset ds_cont_split_mnist --num_epochs $(( 5 * 10)) --batch_size 128 --train_mc_iters 1 --inference_map --federated_learning --n_clients 5 --lr 0.01 --non_iid_split --alpha 1



# Increased Aggregations [5 per task, 10 tasks, total 50 aggregations] || with diff epochs for P-mnist sgd and bgd

# epochs 10

python main.py --num_workers 1 --permute_seed 2019 --seed 1000 --iterations_per_virtual_epc 469 --contpermuted_beta 4 --num_of_permutations $(( 10 - 1 )) --optimizer bgd --nn_arch mnist_simple_net_200width_domainlearning_784input_10cls_1ds --dataset ds_cont_permuted_mnist --num_epochs $(( 10 * 10)) --std_init 0.06 --batch_size 128 --train_mc_iters 10 --inference_map --federated_learning --n_clients 5 --mean_eta 1 --num_aggs_per_task 5 --grad_clip --max_grad_norm 1 

python main.py --num_workers 1 --permute_seed 2019 --seed 1000 --iterations_per_virtual_epc 469 --contpermuted_beta 4 --num_of_permutations $(( 10 - 1 )) --optimizer sgd --nn_arch mnist_simple_net_200width_domainlearning_784input_10cls_1ds --dataset ds_cont_permuted_mnist --num_epochs $(( 10 * 10)) --batch_size 128 --train_mc_iters 1 --inference_map --federated_learning --n_clients 5 --lr 0.01 --num_aggs_per_task 5

# epochs 20

python main.py --num_workers 1 --permute_seed 2019 --seed 1000 --iterations_per_virtual_epc 469 --contpermuted_beta 4 --num_of_permutations $(( 10 - 1 )) --optimizer bgd --nn_arch mnist_simple_net_200width_domainlearning_784input_10cls_1ds --dataset ds_cont_permuted_mnist --num_epochs $(( 10 * 20)) --std_init 0.06 --batch_size 128 --train_mc_iters 10 --inference_map --federated_learning --n_clients 5 --mean_eta 1 --num_aggs_per_task 5 --grad_clip --max_grad_norm 1 

python main.py --num_workers 1 --permute_seed 2019 --seed 1000 --iterations_per_virtual_epc 469 --contpermuted_beta 4 --num_of_permutations $(( 10 - 1 )) --optimizer sgd --nn_arch mnist_simple_net_200width_domainlearning_784input_10cls_1ds --dataset ds_cont_permuted_mnist --num_epochs $(( 10 * 20)) --batch_size 128 --train_mc_iters 1 --inference_map --federated_learning --n_clients 5 --lr 0.01 --num_aggs_per_task 5


# epochs 30

python main.py --num_workers 1 --permute_seed 2019 --seed 1000 --iterations_per_virtual_epc 469 --contpermuted_beta 4 --num_of_permutations $(( 10 - 1 )) --optimizer bgd --nn_arch mnist_simple_net_200width_domainlearning_784input_10cls_1ds --dataset ds_cont_permuted_mnist --num_epochs $(( 10 * 30)) --std_init 0.06 --batch_size 128 --train_mc_iters 10 --inference_map --federated_learning --n_clients 5 --mean_eta 1 --num_aggs_per_task 5 --grad_clip --max_grad_norm 1 

python main.py --num_workers 1 --permute_seed 2019 --seed 1000 --iterations_per_virtual_epc 469 --contpermuted_beta 4 --num_of_permutations $(( 10 - 1 )) --optimizer sgd --nn_arch mnist_simple_net_200width_domainlearning_784input_10cls_1ds --dataset ds_cont_permuted_mnist --num_epochs $(( 10 * 30)) --batch_size 128 --train_mc_iters 1 --inference_map --federated_learning --n_clients 5 --lr 0.01 --num_aggs_per_task 5 


# final for report

# Split mnist dirchilet

python main.py --num_workers 1 --permute_seed 2019 --seed 1000 --iterations_per_virtual_epc 469 --contpermuted_beta 4 --optimizer bgd --nn_arch mnist_simple_net_200width_domainlearning_784input_2cls_1ds --dataset ds_cont_split_mnist --num_epochs $(( 5 * 30)) --std_init 0.06 --batch_size 128 --train_mc_iters 10 --inference_map --federated_learning --n_clients 5 --mean_eta 1 --grad_clip --max_grad_norm 1.0 --non_iid_split --alpha 0.1

python main.py --num_workers 1 --permute_seed 2019 --seed 1000 --iterations_per_virtual_epc 469 --contpermuted_beta 4 --optimizer sgd --nn_arch mnist_simple_net_200width_domainlearning_784input_2cls_1ds --dataset ds_cont_split_mnist --num_epochs $(( 5 * 30)) --batch_size 128 --train_mc_iters 1 --inference_map --federated_learning --n_clients 5 --lr 0.01 --non_iid_split --alpha 0.1


# Dirichlet [perm_5_tasks_non_iid]

python main.py --num_workers 1 --permute_seed 2019 --seed 1000 --iterations_per_virtual_epc 469 --contpermuted_beta 4 --num_of_permutations $(( 5 - 1 )) --optimizer bgd --nn_arch mnist_simple_net_200width_domainlearning_784input_10cls_1ds --dataset ds_cont_permuted_mnist --num_epochs $(( 5 * 30)) --std_init 0.06 --batch_size 128 --train_mc_iters 10 --inference_map --federated_learning --n_clients 5 --mean_eta 1 --grad_clip --max_grad_norm 1  --non_iid --alpha 0.01

python main.py --num_workers 1 --permute_seed 2019 --seed 1000 --iterations_per_virtual_epc 469 --contpermuted_beta 4 --num_of_permutations $(( 5 - 1 )) --optimizer sgd --nn_arch mnist_simple_net_200width_domainlearning_784input_10cls_1ds --dataset ds_cont_permuted_mnist --num_epochs $(( 5 * 30)) --batch_size 128 --train_mc_iters 1 --inference_map --federated_learning --n_clients 5 --lr 0.01 --non_iid --alpha 0.01 


# Dirchilet (Bgd new update)

python main.py --num_workers 1 --permute_seed 2019 --seed 1000 --iterations_per_virtual_epc 469 --contpermuted_beta 4 --num_of_permutations $(( 5 - 1 )) --optimizer bgd_new_update --nn_arch mnist_simple_net_200width_domainlearning_784input_10cls_1ds --dataset ds_cont_permuted_mnist --num_epochs $(( 5 * 30)) --batch_size 128 --train_mc_iters 1 --inference_map --federated_learning --n_clients 5 --lr 0.01 --non_iid --alpha 0.01 --alpha_mg 0.5 


## Experiments with new update

## BGD_NEW (new aggregation and old aggregation) [5 tasks, 30 epochs]

# IID
python main.py --num_workers 1 --permute_seed 2019 --seed 1000 --iterations_per_virtual_epc 469 --contpermuted_beta 4 --num_of_permutations $(( 5 - 1 )) --optimizer bgd_new_update --nn_arch mnist_simple_net_200width_domainlearning_784input_10cls_1ds --dataset ds_cont_permuted_mnist --num_epochs $(( 5 * 30)) --batch_size 128 --train_mc_iters 1 --inference_map --federated_learning --n_clients 5 --lr 0.01 --non_iid --alpha 100000 --alpha_mg 0.5 

# Non-IID
python main.py --num_workers 1 --permute_seed 2019 --seed 1000 --iterations_per_virtual_epc 469 --contpermuted_beta 4 --num_of_permutations $(( 5 - 1 )) --optimizer bgd_new_update --nn_arch mnist_simple_net_200width_domainlearning_784input_10cls_1ds --dataset ds_cont_permuted_mnist --num_epochs $(( 5 * 30)) --batch_size 128 --train_mc_iters 1 --inference_map --federated_learning --n_clients 5 --lr 0.01 --non_iid --alpha 0.01 --alpha_mg 0.5 



