[2024-09-07 22:50:16 Info] Script args: Namespace(dataset='ds_cont_permuted_mnist', nn_arch='mnist_simple_net_200width_domainlearning_784input_10cls_1ds', logname='continous_permuted_mnist_5_tasks', results_dir='ds_cont_permuted_mnist_5_tasks_50_epochs_alpha_0.01_data_type_non_iid_2.0_mean_eta__bgd_new_update_optim_4_beta_with_grad_clip_04:20_08-09-2024', seed=1000, num_workers=8, num_epochs=50, batch_size=128, pruning_percents=[], train_mc_iters=10, std_init=0.05, mean_eta=2.0, permanent_prune_on_epoch=-1, permanent_prune_on_epoch_percent=90, momentum=0.9, lr=0.01, weight_decay=0.0005, test_freq=1, contpermuted_beta=4, optimizer='bgd_new_update', optimizer_params='{}', inference_mc=False, inference_map=True, inference_committee=False, inference_aggsoftmax=False, inference_initstd=False, committee_size=0, test_mc_iters=0, init_params=['{"bias_type":', '"xavier",', '"conv_type":', '"xavier",', '"bn_init":', '"01"}'], desc='', bw_to_rgb=False, permuted_offset=False, labels_trick=False, num_of_permutations=4, iterations_per_virtual_epc=469, separate_labels_space=False, permute_seed=2019, federated_learning=True, n_clients=5, num_aggs_per_task=1, grad_clip=True, max_grad_norm=1.0, non_iid_split=True, alpha=0.01, alpha_mg=0.1)
[2024-09-07 22:50:16 Info] Computer name: iiitd with pytorch version: 2.0.1
[2024-09-07 22:50:16 Info] Arguments are Namespace(dataset='ds_cont_permuted_mnist', nn_arch='mnist_simple_net_200width_domainlearning_784input_10cls_1ds', logname='continous_permuted_mnist_5_tasks', results_dir='ds_cont_permuted_mnist_5_tasks_50_epochs_alpha_0.01_data_type_non_iid_2.0_mean_eta__bgd_new_update_optim_4_beta_with_grad_clip_04:20_08-09-2024', seed=1000, num_workers=8, num_epochs=50, batch_size=128, pruning_percents=[], train_mc_iters=10, std_init=0.05, mean_eta=2.0, permanent_prune_on_epoch=-1, permanent_prune_on_epoch_percent=90, momentum=0.9, lr=0.01, weight_decay=0.0005, test_freq=1, contpermuted_beta=4, optimizer='bgd_new_update', optimizer_params='{}', inference_mc=False, inference_map=True, inference_committee=False, inference_aggsoftmax=False, inference_initstd=False, committee_size=0, test_mc_iters=0, init_params=['{"bias_type":', '"xavier",', '"conv_type":', '"xavier",', '"bn_init":', '"01"}'], desc='', bw_to_rgb=False, permuted_offset=False, labels_trick=False, num_of_permutations=4, iterations_per_virtual_epc=469, separate_labels_space=False, permute_seed=2019, federated_learning=True, n_clients=5, num_aggs_per_task=1, grad_clip=True, max_grad_norm=1.0, non_iid_split=True, alpha=0.01, alpha_mg=0.1)
[2024-09-07 22:51:33 Info] Transformed model to CUDA
[2024-09-07 22:51:33 Info] Initialized 0 Conv2d layers using nn.init.xavier_normal_
[2024-09-07 22:51:33 Info] Initialized 3 linear layers using xavier
[2024-09-07 22:51:33 Info] Initialized 0 bias conv2d layers using nn.init.xavier.noraml_
[2024-09-07 22:51:33 Info] Initialized 3 bias linear layers using xavier
[2024-09-07 22:51:33 Info] Initialized 0 BN layers using weight=1 and bias=0
[2024-09-07 22:51:33 Info] BGD params: {'mean_eta': 2.0, 'std_init': 0.05, 'mc_iters': 10, 'alpha_mg': 0.1}
[2024-09-07 22:51:33 Info] Inference method: {'map'}
[2024-09-07 22:51:33 Info] Number of parameters in the model is 199,210
[2024-09-07 22:51:33 Info] Criterion parameters: type=<class 'torch.nn.modules.loss.CrossEntropyLoss'>
[2024-09-07 22:51:33 Info] BGD params: {'mean_eta': 2.0, 'std_init': 0.05, 'mc_iters': 10, 'alpha_mg': 0.1}
[2024-09-07 22:51:33 Info] Gradient clipping with max_norm being done
[2024-09-07 22:51:34 Info] Training epoch number 1 with dataset number 0
[2024-09-07 22:51:44 Info] Client id 0, Epoch 1, train set, Iter 500 current average loss 0.8834
[2024-09-07 22:51:54 Info] Client id 0, Epoch 1, train set, Iter 1000 current average loss 0.8753
[2024-09-07 22:52:05 Info] Client id 0, Epoch 1, train set, Iter 1500 current average loss 0.8741
[2024-09-07 22:52:16 Info] Client id 0, Epoch 1, train set, Iter 2000 current average loss 0.8713
[2024-09-07 22:52:27 Info] Client id 0, Epoch 1, train set, Iter 2500 current average loss 0.8706
[2024-09-07 22:52:37 Info] Client id 0, Epoch 1, train set, Iter 3000 current average loss 0.8701
[2024-09-07 22:52:48 Info] Client id 0, Epoch 1, train set, Iter 3500 current average loss 0.8694
[2024-09-07 22:52:59 Info] Client id 0, Epoch 1, train set, Iter 4000 current average loss 0.8778
[2024-09-07 22:53:09 Info] Client id 0, Epoch 1, train set, Iter 4500 current average loss 0.8961
[2024-09-07 22:53:20 Info] Client id 0, Epoch 1, train set, Iter 5000 current average loss 0.9155
[2024-09-07 22:53:31 Info] Client id 0, Epoch 1, train set, Iter 5500 current average loss 0.9307
[2024-09-07 22:53:39 Info] Running test set for epoch number 1 for dataset idx 0 using map
[2024-09-07 22:53:39 Info] Stats for test [map] set of size 10000, loss is 2.7333, acc is 8.92%
[2024-09-07 22:53:39 Info] Running test set for epoch number 1 for dataset idx 1 using map
[2024-09-07 22:53:40 Info] Stats for test [map] set of size 10000, loss is 2.9601, acc is 19.98%
[2024-09-07 22:53:40 Info] Running test set for epoch number 1 for dataset idx 2 using map
[2024-09-07 22:53:41 Info] Stats for test [map] set of size 10000, loss is 2.7632, acc is 8.9%
[2024-09-07 22:53:41 Info] Running test set for epoch number 1 for dataset idx 3 using map
[2024-09-07 22:53:41 Info] Stats for test [map] set of size 10000, loss is 2.6699, acc is 8.95%
[2024-09-07 22:53:41 Info] Running test set for epoch number 1 for dataset idx 4 using map
[2024-09-07 22:53:42 Info] Stats for test [map] set of size 10000, loss is 2.8938, acc is 8.92%
[2024-09-07 22:53:42 Info] Average accuracy over all tasks for epoch number 1 for dataset idx 4 using map is 11.134
[2024-09-07 22:53:42 Info] Finished epoch number 1, Took 128 seconds
[2024-09-07 22:53:42 Info] Inference method: {'map'}
[2024-09-07 22:53:42 Info] Number of parameters in the model is 199,210
[2024-09-07 22:53:42 Info] Criterion parameters: type=<class 'torch.nn.modules.loss.CrossEntropyLoss'>
[2024-09-07 22:53:42 Info] BGD params: {'mean_eta': 2.0, 'std_init': 0.05, 'mc_iters': 10, 'alpha_mg': 0.1}
[2024-09-07 22:53:42 Info] Gradient clipping with max_norm being done
[2024-09-07 22:53:42 Info] Training epoch number 1 with dataset number 0
[2024-09-07 22:53:55 Info] Client id 1, Epoch 1, train set, Iter 500 current average loss 0.7252
[2024-09-07 22:54:06 Info] Client id 1, Epoch 1, train set, Iter 1000 current average loss 0.712
[2024-09-07 22:54:16 Info] Client id 1, Epoch 1, train set, Iter 1500 current average loss 0.7085
[2024-09-07 22:54:26 Info] Client id 1, Epoch 1, train set, Iter 2000 current average loss 0.7077
