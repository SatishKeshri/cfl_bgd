[2024-09-06 13:16:09 Info] Script args: Namespace(dataset='ds_cont_permuted_mnist', nn_arch='mnist_simple_net_200width_domainlearning_784input_10cls_1ds', logname='continous_permuted_mnist_5_tasks', results_dir='ds_cont_permuted_mnist_5_tasks_150_epochs_alpha_0.01_data_type_non_iid_2.0_mean_eta__bgd_new_update_optim_4_beta_with_grad_clip_18:46_06-09-2024', seed=1000, num_workers=1, num_epochs=150, batch_size=32, pruning_percents=[], train_mc_iters=10, std_init=0.05, mean_eta=2.0, permanent_prune_on_epoch=-1, permanent_prune_on_epoch_percent=90, momentum=0.9, lr=0.01, weight_decay=0.0005, test_freq=1, contpermuted_beta=4, optimizer='bgd_new_update', optimizer_params='{}', inference_mc=False, inference_map=True, inference_committee=False, inference_aggsoftmax=False, inference_initstd=False, committee_size=0, test_mc_iters=0, init_params=['{"bias_type":', '"xavier",', '"conv_type":', '"xavier",', '"bn_init":', '"01"}'], desc='', bw_to_rgb=False, permuted_offset=False, labels_trick=False, num_of_permutations=4, iterations_per_virtual_epc=469, separate_labels_space=False, permute_seed=2019, federated_learning=True, n_clients=5, num_aggs_per_task=10, grad_clip=True, max_grad_norm=1.0, non_iid_split=True, alpha=0.01, alpha_mg=0.1)
[2024-09-06 13:16:09 Info] Computer name: iiitd with pytorch version: 2.0.1
[2024-09-06 13:16:09 Info] Arguments are Namespace(dataset='ds_cont_permuted_mnist', nn_arch='mnist_simple_net_200width_domainlearning_784input_10cls_1ds', logname='continous_permuted_mnist_5_tasks', results_dir='ds_cont_permuted_mnist_5_tasks_150_epochs_alpha_0.01_data_type_non_iid_2.0_mean_eta__bgd_new_update_optim_4_beta_with_grad_clip_18:46_06-09-2024', seed=1000, num_workers=1, num_epochs=150, batch_size=32, pruning_percents=[], train_mc_iters=10, std_init=0.05, mean_eta=2.0, permanent_prune_on_epoch=-1, permanent_prune_on_epoch_percent=90, momentum=0.9, lr=0.01, weight_decay=0.0005, test_freq=1, contpermuted_beta=4, optimizer='bgd_new_update', optimizer_params='{}', inference_mc=False, inference_map=True, inference_committee=False, inference_aggsoftmax=False, inference_initstd=False, committee_size=0, test_mc_iters=0, init_params=['{"bias_type":', '"xavier",', '"conv_type":', '"xavier",', '"bn_init":', '"01"}'], desc='', bw_to_rgb=False, permuted_offset=False, labels_trick=False, num_of_permutations=4, iterations_per_virtual_epc=469, separate_labels_space=False, permute_seed=2019, federated_learning=True, n_clients=5, num_aggs_per_task=10, grad_clip=True, max_grad_norm=1.0, non_iid_split=True, alpha=0.01, alpha_mg=0.1)
[2024-09-06 13:18:13 Info] Transformed model to CUDA
[2024-09-06 13:18:13 Info] Initialized 0 Conv2d layers using nn.init.xavier_normal_
[2024-09-06 13:18:13 Info] Initialized 3 linear layers using xavier
[2024-09-06 13:18:13 Info] Initialized 0 bias conv2d layers using nn.init.xavier.noraml_
[2024-09-06 13:18:13 Info] Initialized 3 bias linear layers using xavier
[2024-09-06 13:18:13 Info] Initialized 0 BN layers using weight=1 and bias=0
[2024-09-06 13:18:13 Info] BGD params: {'mean_eta': 2.0, 'std_init': 0.05, 'mc_iters': 10, 'alpha_mg': 0.1}
[2024-09-06 13:18:13 Info] Inference method: {'map'}
[2024-09-06 13:18:13 Info] Number of parameters in the model is 199,210
[2024-09-06 13:18:13 Info] Criterion parameters: type=<class 'torch.nn.modules.loss.CrossEntropyLoss'>
[2024-09-06 13:18:13 Info] BGD params: {'mean_eta': 2.0, 'std_init': 0.05, 'mc_iters': 10, 'alpha_mg': 0.1}
[2024-09-06 13:18:13 Info] Gradient clipping with max_norm being done
[2024-09-06 13:18:13 Info] Training epoch number 1 with dataset number 0
[2024-09-06 13:18:58 Info] Client id 0, Epoch 1, train set, Iter 500 current average loss 0.7788
[2024-09-06 13:19:41 Info] Client id 0, Epoch 1, train set, Iter 1000 current average loss 0.7764
[2024-09-06 13:20:31 Info] Client id 0, Epoch 1, train set, Iter 1500 current average loss 0.7761
