{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'17:57_25-06-2024'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import pytz\n",
    "\n",
    "\n",
    "IST = pytz.timezone('Asia/Kolkata')\n",
    "\n",
    "current_time = datetime.now(IST).strftime(\"%H:%M_%d-%m-%Y\")\n",
    "current_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4732.94677734375\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "\n",
    "model = resnet18()\n",
    "\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for layer in model.parameters():\n",
    "        total+=torch.sum(layer)\n",
    "\n",
    "print(total.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 4, 5]) tensor([ 6,  7,  9, 10])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import copy\n",
    "\n",
    "a = torch.tensor([1,2,4,5])\n",
    "\n",
    "b = copy.deepcopy(a)\n",
    "\n",
    "b.add_(5)\n",
    "\n",
    "print(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 9, 18, 27])\n",
      "tensor([ 9, 18, 27])\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "\n",
    "a = torch.tensor([1,2,3])\n",
    "\n",
    "b = a\n",
    "\n",
    "b.mul_(9)\n",
    "\n",
    "print(a)\n",
    "print(b)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import codecs\n",
    "from torch.distributions.categorical import Categorical\n",
    "import torch.utils.data as data\n",
    "from PIL import Image\n",
    "import errno\n",
    "\n",
    "\n",
    "def _reduce_class(set, classes, train, preserve_label_space=True):\n",
    "    if classes is None:\n",
    "        return\n",
    "\n",
    "    new_class_idx = {}\n",
    "    for c in classes:\n",
    "        new_class_idx[c] = new_class_idx.__len__()\n",
    "\n",
    "    new_data = []\n",
    "    new_labels = []\n",
    "    if train:\n",
    "        all_data = set.train_data\n",
    "        labels = set.train_labels\n",
    "    else:\n",
    "        all_data = set.test_data\n",
    "        labels = set.test_labels\n",
    "\n",
    "    for data, label in zip(all_data, labels):\n",
    "        if type(label) == int:\n",
    "            label_val = label\n",
    "        else:\n",
    "            label_val = label.item()\n",
    "        if label_val in classes:\n",
    "            new_data.append(data)\n",
    "            if preserve_label_space:\n",
    "                new_labels += [label_val]\n",
    "            else:\n",
    "                new_labels += [new_class_idx[label_val]]\n",
    "    if type(new_data[0]) == np.ndarray:\n",
    "        new_data = np.array(new_data)\n",
    "    elif type(new_data[0]) == torch.Tensor:\n",
    "        new_data = torch.stack(new_data)\n",
    "    else:\n",
    "        assert False, \"Reduce class not supported\"\n",
    "    if train:\n",
    "        set.train_data = new_data\n",
    "        set.train_labels = new_labels\n",
    "    else:\n",
    "        set.test_data = new_data\n",
    "        set.test_labels = new_labels\n",
    "\n",
    "\n",
    "class Permutation(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    A dataset wrapper that permute the position of features\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, permute_idx, target_offset):\n",
    "        super(Permutation,self).__init__()\n",
    "        self.dataset = dataset\n",
    "        self.permute_idx = permute_idx\n",
    "        self.target_offset = target_offset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img, target = self.dataset[index]\n",
    "        target = target + self.target_offset\n",
    "        shape = img.size()\n",
    "        img = img.view(-1)[self.permute_idx].view(shape)\n",
    "        return img, target\n",
    "\n",
    "\n",
    "class DatasetsLoaders:\n",
    "    def __init__(self, dataset, batch_size=4, num_workers=4, pin_memory=True,**kwargs):\n",
    "        # print(\"kwargs in datasetloaders - \",kwargs)\n",
    "        self.dataset_name = dataset\n",
    "        self.valid_loader = None\n",
    "        self.num_workers = num_workers\n",
    "        if self.num_workers is None:\n",
    "            self.num_workers = 4\n",
    "\n",
    "        self.random_erasing = kwargs.get(\"random_erasing\", False)\n",
    "        self.reduce_classes = kwargs.get(\"reduce_classes\", None)\n",
    "        self.permute = kwargs.get(\"permute\", False)\n",
    "        self.target_offset = kwargs.get(\"target_offset\", 0)\n",
    "\n",
    "        pin_memory = pin_memory if torch.cuda.is_available() else False\n",
    "        self.batch_size = batch_size\n",
    "        cifar10_mean = (0.5, 0.5, 0.5)\n",
    "        cifar10_std = (0.5, 0.5, 0.5)\n",
    "        cifar100_mean = (0.5070, 0.4865, 0.4409)\n",
    "        cifar100_std = (0.2673, 0.2564, 0.2761)\n",
    "        mnist_mean = [33.318421449829934]\n",
    "        mnist_std = [78.56749083061408]\n",
    "        fashionmnist_mean = [73.14654541015625]\n",
    "        fashionmnist_std = [89.8732681274414]\n",
    "\n",
    "        if dataset == \"CIFAR10\":\n",
    "            # CIFAR10:\n",
    "            #   type               : uint8\n",
    "            #   shape              : train_set.train_data.shape (50000, 32, 32, 3)\n",
    "            #   test data shape    : (10000, 32, 32, 3)\n",
    "            #   number of channels : 3\n",
    "            #   Mean per channel   : train_set.train_data[:,:,:,0].mean() 125.306918046875\n",
    "            #                        train_set.train_data[:,:,:,1].mean() 122.95039414062499\n",
    "            #                        train_set.train_data[:,:,:,2].mean() 113.86538318359375\n",
    "            #   Std per channel   :  train_set.train_data[:, :, :, 0].std() 62.993219278136884\n",
    "            #                        train_set.train_data[:, :, :, 1].std() 62.088707640014213\n",
    "            #                        train_set.train_data[:, :, :, 2].std() 66.704899640630913\n",
    "            self.mean = cifar10_mean\n",
    "            self.std = cifar10_std\n",
    "\n",
    "            transform_train = transforms.Compose([\n",
    "                transforms.RandomCrop(32, padding=4),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "            ])\n",
    "\n",
    "            transform_test = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "            ])\n",
    "\n",
    "            self.train_set = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                                          download=True, transform=transform_train)\n",
    "            self.train_loader = torch.utils.data.DataLoader(self.train_set, batch_size=self.batch_size,\n",
    "                                                            shuffle=True, num_workers=self.num_workers,\n",
    "                                                            pin_memory=pin_memory)\n",
    "\n",
    "            self.test_set = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                                         download=True, transform=transform_test)\n",
    "            self.test_loader = torch.utils.data.DataLoader(self.test_set, batch_size=self.batch_size,\n",
    "                                                           shuffle=False, num_workers=self.num_workers,\n",
    "                                                           pin_memory=pin_memory)\n",
    "        if dataset == \"CIFAR100\":\n",
    "            # CIFAR100:\n",
    "            #   type               : uint8\n",
    "            #   shape              : train_set.train_data.shape (50000, 32, 32, 3)\n",
    "            #   test data shape    : (10000, 32, 32, 3)\n",
    "            #   number of channels : 3\n",
    "            #   Mean per channel   : train_set.train_data[:,:,:,0].mean() 129.304165605/255=0.5070\n",
    "            #                        train_set.train_data[:,:,:,1].mean() 124.069962695/255=0.4865\n",
    "            #                        train_set.train_data[:,:,:,2].mean() 112.434050059/255=0.4409\n",
    "            #   Std per channel   :  train_set.train_data[:, :, :, 0].std() 68.1702428992/255=0.2673\n",
    "            #                        train_set.train_data[:, :, :, 1].std() 65.3918080439/255=0.2564\n",
    "            #                        train_set.train_data[:, :, :, 2].std() 70.418370188/255=0.2761\n",
    "\n",
    "            self.mean = cifar100_mean\n",
    "            self.std = cifar100_std\n",
    "            transform = transforms.Compose(\n",
    "                [transforms.ToTensor(),\n",
    "                 transforms.Normalize(self.mean, self.std)])\n",
    "\n",
    "            self.train_set = torchvision.datasets.CIFAR100(root='./data', train=True,\n",
    "                                                           download=True, transform=transform)\n",
    "            _reduce_class(self.train_set, self.reduce_classes, train=True,\n",
    "                          preserve_label_space=kwargs.get(\"preserve_label_space\"))\n",
    "            self.train_loader = torch.utils.data.DataLoader(self.train_set, batch_size=self.batch_size,\n",
    "                                                            shuffle=True, num_workers=self.num_workers,\n",
    "                                                            pin_memory=pin_memory)\n",
    "\n",
    "            self.test_set = torchvision.datasets.CIFAR100(root='./data', train=False,\n",
    "                                                          download=True, transform=transform)\n",
    "            _reduce_class(self.test_set, self.reduce_classes, train=False,\n",
    "                          preserve_label_space=kwargs.get(\"preserve_label_space\"))\n",
    "            self.test_loader = torch.utils.data.DataLoader(self.test_set, batch_size=self.batch_size,\n",
    "                                                           shuffle=False, num_workers=self.num_workers,\n",
    "                                                           pin_memory=pin_memory)\n",
    "        if dataset == \"MNIST\":\n",
    "            # MNIST:\n",
    "            #   type               : torch.ByteTensor\n",
    "            #   shape              : train_set.train_data.shape torch.Size([60000, 28, 28])\n",
    "            #   test data shape    : [10000, 28, 28]\n",
    "            #   number of channels : 1\n",
    "            #   Mean per channel   : 33.318421449829934\n",
    "            #   Std per channel    : 78.56749083061408\n",
    "\n",
    "            # Transforms\n",
    "            self.mean = mnist_mean\n",
    "            self.std = mnist_std\n",
    "            if kwargs.get(\"pad_to_32\", False):\n",
    "                transform = transforms.Compose(\n",
    "                    [transforms.Pad(2, fill=0, padding_mode='constant'),\n",
    "                     transforms.ToTensor(),\n",
    "                     transforms.Normalize(mean=(0.1000,), std=(0.2752,))])\n",
    "            else:\n",
    "                transform = transforms.Compose(\n",
    "                    [transforms.ToTensor()])\n",
    "\n",
    "            # Create train set\n",
    "            self.train_set = torchvision.datasets.MNIST(root='./data', train=True,\n",
    "                                                        download=True, transform=transform)\n",
    "            if kwargs.get(\"permutation\", False):\n",
    "                # Permute if permutation is provided\n",
    "                self.train_set = Permutation(torchvision.datasets.MNIST(root='./data', train=True,\n",
    "                                                                        download=True, transform=transform),\n",
    "                                             kwargs.get(\"permutation\", False), self.target_offset)\n",
    "            # Reduce classes if necessary\n",
    "            _reduce_class(self.train_set, self.reduce_classes, train=True,\n",
    "                          preserve_label_space=kwargs.get(\"preserve_label_space\"))\n",
    "            # Remap labels\n",
    "            if kwargs.get(\"labels_remapping\", False):\n",
    "                labels_remapping = kwargs.get(\"labels_remapping\", False)\n",
    "                for lbl_idx in range(len(self.train_set.train_labels)):\n",
    "                    self.train_set.train_labels[lbl_idx] = labels_remapping[self.train_set.train_labels[lbl_idx]]\n",
    "\n",
    "            self.train_loader = torch.utils.data.DataLoader(self.train_set, batch_size=self.batch_size,\n",
    "                                                            shuffle=True, num_workers=self.num_workers,\n",
    "                                                            pin_memory=pin_memory)\n",
    "\n",
    "            # Create test set\n",
    "            self.test_set = torchvision.datasets.MNIST(root='./data', train=False,\n",
    "                                                       download=True, transform=transform)\n",
    "            if kwargs.get(\"permutation\", False):\n",
    "                # Permute if permutation is provided\n",
    "                self.test_set = Permutation(torchvision.datasets.MNIST(root='./data', train=False,\n",
    "                                                                        download=True, transform=transform),\n",
    "                                             kwargs.get(\"permutation\", False), self.target_offset)\n",
    "            # Reduce classes if necessary\n",
    "            _reduce_class(self.test_set, self.reduce_classes, train=False,\n",
    "                          preserve_label_space=kwargs.get(\"preserve_label_space\"))\n",
    "            # Remap labels\n",
    "            if kwargs.get(\"labels_remapping\", False):\n",
    "                labels_remapping = kwargs.get(\"labels_remapping\", False)\n",
    "                for lbl_idx in range(len(self.test_set.test_labels)):\n",
    "                    self.test_set.test_labels[lbl_idx] = labels_remapping[self.test_set.test_labels[lbl_idx]]\n",
    "\n",
    "            self.test_loader = torch.utils.data.DataLoader(self.test_set, batch_size=self.batch_size,\n",
    "                                                           shuffle=False, num_workers=self.num_workers,\n",
    "                                                           pin_memory=pin_memory)\n",
    "        \n",
    "        \n",
    "        if dataset == \"CONTPERMUTEDPADDEDMNIST\" or dataset == \"CONTPERMUTEDMNIST\":\n",
    "\n",
    "            # if dataset == \"CONTPERMUTEDPADDEDMNIST\":\n",
    "            #     transform = transforms.Compose(\n",
    "            #         [transforms.Pad(2, fill=0, padding_mode='constant'),\n",
    "            #         transforms.ToTensor(),\n",
    "            #         transforms.Normalize(mean=(0.1000,), std=(0.2752,))])\n",
    "\n",
    "            if dataset == \"CONTPERMUTEDPADDEDMNIST\":\n",
    "                transform = transforms.Compose(\n",
    "                    [transforms.Pad(2, fill=0, padding_mode='constant'),\n",
    "                    transforms.ToTensor()])\n",
    "            \n",
    "            if dataset == \"CONTPERMUTEDMNIST\":\n",
    "                transform = transforms.Compose(\n",
    "                    [transforms.ToTensor()])\n",
    "\n",
    "            # Original MNIST\n",
    "            tasks_datasets = [torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)]\n",
    "            tasks_samples_indices = [torch.tensor(range(len(tasks_datasets[0])), dtype=torch.int32)]\n",
    "            total_len = len(tasks_datasets[0])\n",
    "            test_loaders = [torch.utils.data.DataLoader(torchvision.datasets.MNIST(root='./data', train=False,\n",
    "                                                                                   download=True, transform=transform),\n",
    "                                                        batch_size=self.batch_size, shuffle=False,\n",
    "                                                        num_workers=self.num_workers, pin_memory=pin_memory)]\n",
    "            self.num_of_permutations = len(kwargs.get(\"all_permutation\"))\n",
    "            all_permutation = kwargs.get(\"all_permutation\", None)\n",
    "            for p_idx in range(self.num_of_permutations):\n",
    "                # Create permuation\n",
    "                permutation = all_permutation[p_idx]\n",
    "\n",
    "                # Add train set:\n",
    "                tasks_datasets.append(Permutation(torchvision.datasets.MNIST(root='./data', train=True,\n",
    "                                                                             download=True, transform=transform),\n",
    "                                                  permutation, target_offset=0))\n",
    "\n",
    "                tasks_samples_indices.append(torch.tensor(range(total_len,\n",
    "                                                                total_len + len(tasks_datasets[-1])\n",
    "                                                                ), dtype=torch.int32))\n",
    "                total_len += len(tasks_datasets[-1])\n",
    "                # Add test set:\n",
    "                test_set = Permutation(torchvision.datasets.MNIST(root='./data', train=False,\n",
    "                                                                  download=True, transform=transform),\n",
    "                                       permutation, self.target_offset)\n",
    "                test_loaders.append(torch.utils.data.DataLoader(test_set, batch_size=self.batch_size,\n",
    "                                                                shuffle=False, num_workers=self.num_workers,\n",
    "                                                                pin_memory=pin_memory))\n",
    "            self.test_loader = test_loaders\n",
    "            # Concat datasets\n",
    "            total_iters = kwargs.get(\"total_iters\", None)\n",
    "\n",
    "            assert total_iters is not None\n",
    "            beta = kwargs.get(\"contpermuted_beta\", 3)\n",
    "            all_datasets = torch.utils.data.ConcatDataset(tasks_datasets)\n",
    "\n",
    "            # Create probabilities of tasks over iterations\n",
    "            self.tasks_probs_over_iterations = [_create_task_probs(total_iters, self.num_of_permutations+1, task_id,\n",
    "                                                                    beta=beta)[0] for task_id in\n",
    "                                                 range(self.num_of_permutations+1)]\n",
    "            \n",
    "            normalize_probs = torch.zeros_like(self.tasks_probs_over_iterations[0])\n",
    "            for probs in self.tasks_probs_over_iterations:\n",
    "                normalize_probs.add_(probs)\n",
    "            for probs in self.tasks_probs_over_iterations:\n",
    "                probs.div_(normalize_probs)\n",
    "            self.tasks_probs_over_iterations = torch.cat(self.tasks_probs_over_iterations).view(-1, self.tasks_probs_over_iterations[0].shape[0])\n",
    "            tasks_probs_over_iterations_lst = []\n",
    "            for col in range(self.tasks_probs_over_iterations.shape[1]):\n",
    "                tasks_probs_over_iterations_lst.append(self.tasks_probs_over_iterations[:, col])\n",
    "            self.tasks_probs_over_iterations = tasks_probs_over_iterations_lst\n",
    "\n",
    "            train_sampler = ContinuousMultinomialSampler(data_source=all_datasets, samples_in_batch=self.batch_size,\n",
    "                                                         tasks_samples_indices=tasks_samples_indices,\n",
    "                                                         tasks_probs_over_iterations=\n",
    "                                                             self.tasks_probs_over_iterations,\n",
    "                                                         num_of_batches=kwargs.get(\"iterations_per_virtual_epc\", 1))\n",
    "            \n",
    "            self.train_loader = torch.utils.data.DataLoader(all_datasets, batch_size=self.batch_size,\n",
    "                                                            num_workers=self.num_workers, sampler=train_sampler, pin_memory=pin_memory)\n",
    "\n",
    "            print('I am here')\n",
    "        \n",
    "\n",
    "class ContinuousMultinomialSampler(torch.utils.data.Sampler):\n",
    "    r\"\"\"Samples elements randomly. If without replacement, then sample from a shuffled dataset.\n",
    "    If with replacement, then user can specify ``num_samples`` to draw.\n",
    "    self.tasks_probs_over_iterations is the probabilities of tasks over iterations.\n",
    "    self.samples_distribution_over_time is the actual distribution of samples over iterations\n",
    "                                            (the result of sampling from self.tasks_probs_over_iterations).\n",
    "    Arguments:\n",
    "        data_source (Dataset): dataset to sample from\n",
    "        num_samples (int): number of samples to draw, default=len(dataset)\n",
    "        replacement (bool): samples are drawn with replacement if ``True``, default=False\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_source, samples_in_batch=128, num_of_batches=69, tasks_samples_indices=None,\n",
    "                 tasks_probs_over_iterations=None):\n",
    "        self.data_source = data_source\n",
    "        assert tasks_samples_indices is not None, \"Must provide tasks_samples_indices - a list of tensors,\" \\\n",
    "                                                  \"each item in the list corrosponds to a task, each item of the \" \\\n",
    "                                                  \"tensor corrosponds to index of sample of this task\"\n",
    "        self.tasks_samples_indices = tasks_samples_indices\n",
    "        self.num_of_tasks = len(self.tasks_samples_indices)\n",
    "        assert tasks_probs_over_iterations is not None, \"Must provide tasks_probs_over_iterations - a list of \" \\\n",
    "                                                         \"probs per iteration\"\n",
    "        assert all([isinstance(probs, torch.Tensor) and len(probs) == self.num_of_tasks for\n",
    "                    probs in tasks_probs_over_iterations]), \"All probs must be tensors of len\" \\\n",
    "                                                              + str(self.num_of_tasks) + \", first tensor type is \" \\\n",
    "                                                              + str(type(tasks_probs_over_iterations[0])) + \", and \" \\\n",
    "                                                              \" len is \" + str(len(tasks_probs_over_iterations[0]))\n",
    "        self.tasks_probs_over_iterations = tasks_probs_over_iterations\n",
    "        self.current_iteration = 0\n",
    "\n",
    "        self.samples_in_batch = samples_in_batch\n",
    "        self.num_of_batches = num_of_batches\n",
    "\n",
    "        # Create the samples_distribution_over_time\n",
    "        self.samples_distribution_over_time = [[] for _ in range(self.num_of_tasks)]\n",
    "        self.iter_indices_per_iteration = []\n",
    "\n",
    "        if not isinstance(self.samples_in_batch, int) or self.samples_in_batch <= 0:\n",
    "            raise ValueError(\"num_samples should be a positive integeral \"\n",
    "                             \"value, but got num_samples={}\".format(self.samples_in_batch))\n",
    "    \n",
    "    def generate_iters_indices(self, num_of_iters):\n",
    "        from_iter = len(self.iter_indices_per_iteration)\n",
    "        for iter_num in range(from_iter, from_iter+num_of_iters):\n",
    "\n",
    "            # Get random number of samples per task (according to iteration distribution)\n",
    "            tsks = Categorical(probs=self.tasks_probs_over_iterations[iter_num]).sample(torch.Size([self.samples_in_batch]))\n",
    "            # Generate samples indices for iter_num\n",
    "            iter_indices = torch.zeros(0, dtype=torch.int32)\n",
    "            for task_idx in range(self.num_of_tasks):\n",
    "                if self.tasks_probs_over_iterations[iter_num][task_idx] > 0:\n",
    "                    num_samples_from_task = (tsks == task_idx).sum().item()\n",
    "                    self.samples_distribution_over_time[task_idx].append(num_samples_from_task)\n",
    "                    # Randomize indices for each task (to allow creation of random task batch)\n",
    "                    tasks_inner_permute = np.random.permutation(len(self.tasks_samples_indices[task_idx]))\n",
    "                    rand_indices_of_task = tasks_inner_permute[:num_samples_from_task]\n",
    "                    iter_indices = torch.cat([iter_indices, self.tasks_samples_indices[task_idx][rand_indices_of_task]])\n",
    "                else:\n",
    "                    self.samples_distribution_over_time[task_idx].append(0)\n",
    "            self.iter_indices_per_iteration.append(iter_indices.tolist())\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.generate_iters_indices(self.num_of_batches)\n",
    "        self.current_iteration += self.num_of_batches\n",
    "        return iter([item for sublist in self.iter_indices_per_iteration[self.current_iteration - self.num_of_batches:self.current_iteration] for item in sublist])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples_in_batch)\n",
    "\n",
    "\n",
    "def _get_linear_line(start, end, direction=\"up\"):\n",
    "    if direction == \"up\":\n",
    "        return torch.FloatTensor([(i - start)/(end-start) for i in range(start, end)])\n",
    "    return torch.FloatTensor([1 - ((i - start) / (end - start)) for i in range(start, end)])\n",
    "\n",
    "\n",
    "def _create_task_probs(iters, tasks, task_id, beta=3):\n",
    "    if beta <= 1:\n",
    "        peak_start = int((task_id/tasks)*iters)\n",
    "        peak_end = int(((task_id + 1) / tasks)*iters)\n",
    "        start = peak_start\n",
    "        end = peak_end\n",
    "    else:\n",
    "        start = max(int(((beta*task_id - 1)*iters)/(beta*tasks)), 0)\n",
    "        peak_start = int(((beta*task_id + 1)*iters)/(beta*tasks))\n",
    "        peak_end = int(((beta * task_id + (beta - 1)) * iters) / (beta * tasks))\n",
    "        end = min(int(((beta * task_id + (beta + 1)) * iters) / (beta * tasks)), iters)\n",
    "\n",
    "    #This is a probability dist for each task across iterations\n",
    "    probs = torch.zeros(iters, dtype=torch.float)\n",
    "\n",
    "    if task_id == 0:\n",
    "        probs[start:peak_start].add_(1)\n",
    "    else:\n",
    "        probs[start:peak_start] = _get_linear_line(start, peak_start, direction=\"up\")\n",
    "    probs[peak_start:peak_end].add_(1)\n",
    "    if task_id == tasks - 1:\n",
    "        probs[peak_end:end].add_(1)\n",
    "    else:\n",
    "        probs[peak_end:end] = _get_linear_line(peak_end, end, direction=\"down\")\n",
    "    \n",
    "    # with open('probs.txt','w') as f:\n",
    "    #     f.write(str(probs.numpy().tolist()))\n",
    "\n",
    "    return probs,end\n",
    "\n",
    "\n",
    "###\n",
    "# NotMNIST\n",
    "###\n",
    "class NOTMNIST(data.Dataset):\n",
    "    \"\"\"`MNIST <http://yann.lecun.com/exdb/mnist/>`_ Dataset.\n",
    "\n",
    "    Args:\n",
    "        root (string): Root directory of dataset where ``processed/training.pt``\n",
    "            and  ``processed/test.pt`` exist.\n",
    "        train (bool, optional): If True, creates dataset from ``training.pt``,\n",
    "            otherwise from ``test.pt``.\n",
    "        download (bool, optional): If true, downloads the dataset from the internet and\n",
    "            puts it in root directory. If dataset is already downloaded, it is not\n",
    "            downloaded again.\n",
    "        transform (callable, optional): A function/transform that  takes in an PIL image\n",
    "            and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
    "        target_transform (callable, optional): A function/transform that takes in the\n",
    "            target and transforms it.\n",
    "    \"\"\"\n",
    "    urls = [\n",
    "        'https://github.com/davidflanagan/notMNIST-to-MNIST/raw/master/t10k-images-idx3-ubyte.gz',\n",
    "        'https://github.com/davidflanagan/notMNIST-to-MNIST/raw/master/t10k-labels-idx1-ubyte.gz',\n",
    "        'https://github.com/davidflanagan/notMNIST-to-MNIST/raw/master/train-images-idx3-ubyte.gz',\n",
    "        'https://github.com/davidflanagan/notMNIST-to-MNIST/raw/master/train-labels-idx1-ubyte.gz',\n",
    "    ]\n",
    "    raw_folder = 'raw'\n",
    "    processed_folder = 'processed'\n",
    "    training_file = 'training.pt'\n",
    "    test_file = 'test.pt'\n",
    "\n",
    "    def __init__(self, root, train=True, transform=None, target_transform=None, download=False):\n",
    "        self.root = os.path.expanduser(root)\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.train = train  # training set or test set\n",
    "\n",
    "        if download:\n",
    "            self.download()\n",
    "\n",
    "        if not self._check_exists():\n",
    "            raise RuntimeError('Dataset not found.' +\n",
    "                               ' You can use download=True to download it')\n",
    "\n",
    "        if self.train:\n",
    "            self.train_data, self.train_labels = torch.load(\n",
    "                os.path.join(self.root, self.processed_folder, self.training_file))\n",
    "        else:\n",
    "            self.test_data, self.test_labels = torch.load(\n",
    "                os.path.join(self.root, self.processed_folder, self.test_file))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is index of the target class.\n",
    "        \"\"\"\n",
    "        if self.train:\n",
    "            img, target = self.train_data[index], self.train_labels[index]\n",
    "        else:\n",
    "            img, target = self.test_data[index], self.test_labels[index]\n",
    "\n",
    "        # doing this so that it is consistent with all other datasets\n",
    "        # to return a PIL Image\n",
    "        img = Image.fromarray(img.numpy(), mode='L')\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.train:\n",
    "            return len(self.train_data)\n",
    "        else:\n",
    "            return len(self.test_data)\n",
    "\n",
    "    def _check_exists(self):\n",
    "        return os.path.exists(os.path.join(self.root, self.processed_folder, self.training_file)) and \\\n",
    "            os.path.exists(os.path.join(self.root, self.processed_folder, self.test_file))\n",
    "\n",
    "    def download(self):\n",
    "        \"\"\"Download the MNIST data if it doesn't exist in processed_folder already.\"\"\"\n",
    "        from six.moves import urllib\n",
    "        import gzip\n",
    "\n",
    "        if self._check_exists():\n",
    "            return\n",
    "\n",
    "        # download files\n",
    "        try:\n",
    "            os.makedirs(os.path.join(self.root, self.raw_folder))\n",
    "            os.makedirs(os.path.join(self.root, self.processed_folder))\n",
    "        except OSError as e:\n",
    "            if e.errno == errno.EEXIST:\n",
    "                pass\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "        for url in self.urls:\n",
    "            print('Downloading ' + url)\n",
    "            data = urllib.request.urlopen(url)\n",
    "            filename = url.rpartition('/')[2]\n",
    "            file_path = os.path.join(self.root, self.raw_folder, filename)\n",
    "            with open(file_path, 'wb') as f:\n",
    "                f.write(data.read())\n",
    "            with open(file_path.replace('.gz', ''), 'wb') as out_f, \\\n",
    "                    gzip.GzipFile(file_path) as zip_f:\n",
    "                out_f.write(zip_f.read())\n",
    "            os.unlink(file_path)\n",
    "\n",
    "        # process and save as torch files\n",
    "        print('Processing...')\n",
    "\n",
    "        training_set = (\n",
    "            self.read_image_file(os.path.join(self.root, self.raw_folder, 'train-images-idx3-ubyte')),\n",
    "            self.read_label_file(os.path.join(self.root, self.raw_folder, 'train-labels-idx1-ubyte'))\n",
    "        )\n",
    "        test_set = (\n",
    "            self.read_image_file(os.path.join(self.root, self.raw_folder, 't10k-images-idx3-ubyte')),\n",
    "            self.read_label_file(os.path.join(self.root, self.raw_folder, 't10k-labels-idx1-ubyte'))\n",
    "        )\n",
    "        with open(os.path.join(self.root, self.processed_folder, self.training_file), 'wb') as f:\n",
    "            torch.save(training_set, f)\n",
    "        with open(os.path.join(self.root, self.processed_folder, self.test_file), 'wb') as f:\n",
    "            torch.save(test_set, f)\n",
    "\n",
    "        print('Done!')\n",
    "\n",
    "    def __repr__(self):\n",
    "        fmt_str = 'Dataset ' + self.__class__.__name__ + '\\n'\n",
    "        fmt_str += '    Number of datapoints: {}\\n'.format(self.__len__())\n",
    "        tmp = 'train' if self.train is True else 'test'\n",
    "        fmt_str += '    Split: {}\\n'.format(tmp)\n",
    "        fmt_str += '    Root Location: {}\\n'.format(self.root)\n",
    "        tmp = '    Transforms (if any): '\n",
    "        fmt_str += '{0}{1}\\n'.format(tmp, self.transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
    "        tmp = '    Target Transforms (if any): '\n",
    "        fmt_str += '{0}{1}'.format(tmp, self.target_transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
    "        return fmt_str\n",
    "\n",
    "    @staticmethod\n",
    "    def get_int(b):\n",
    "        return int(codecs.encode(b, 'hex'), 16)\n",
    "\n",
    "    def read_label_file(self, path):\n",
    "        with open(path, 'rb') as f:\n",
    "            data = f.read()\n",
    "            assert self.get_int(data[:4]) == 2049\n",
    "            length = self.get_int(data[4:8])\n",
    "            parsed = np.frombuffer(data, dtype=np.uint8, offset=8)\n",
    "            return torch.from_numpy(parsed).view(length).long()\n",
    "\n",
    "    def read_image_file(self, path):\n",
    "        with open(path, 'rb') as f:\n",
    "            data = f.read()\n",
    "            assert self.get_int(data[:4]) == 2051\n",
    "            length = self.get_int(data[4:8])\n",
    "            num_rows = self.get_int(data[8:12])\n",
    "            num_cols = self.get_int(data[12:16])\n",
    "            images = []\n",
    "            parsed = np.frombuffer(data, dtype=np.uint8, offset=16)\n",
    "            return torch.from_numpy(parsed).view(length, num_rows, num_cols)\n",
    "\n",
    "\n",
    "###########################################################################\n",
    "# Callable datasets\n",
    "###########################################################################\n",
    "\n",
    "\n",
    "def ds_split_mnist(**kwargs):\n",
    "    \"\"\"\n",
    "    Split MNIST dataset. Consists of 5 tasks: digits 0 & 1, 2 & 3, 4 & 5, 6 & 7, and 8 & 9.\n",
    "    :param batch_size: batch size\n",
    "           num_workers: num of workers\n",
    "           pad_to_32: If true, will pad digits to size 32x32 and normalize to zero mean and unit variance.\n",
    "           separate_labels_space: If true, each task will have its own label space (e.g. 01, 23 etc.).\n",
    "                                  If false, all tasks will have label space of 0,1 only.\n",
    "    :return: Tuple with two lists.\n",
    "             First list of the tuple is a list of 5 train loaders, each loader is a task.\n",
    "             Second list of the tuple is a list of 5 test loaders, each loader is a task.\n",
    "    \"\"\"\n",
    "    classes_lst = [\n",
    "        [0, 1],\n",
    "        [2, 3],\n",
    "        [4, 5],\n",
    "        [6, 7],\n",
    "        [8, 9]\n",
    "    ]\n",
    "    dataset = [DatasetsLoaders(\"MNIST\", batch_size=kwargs.get(\"batch_size\", 128),\n",
    "                               num_workers=kwargs.get(\"num_workers\", 1),\n",
    "                               reduce_classes=cl, pad_to_32=kwargs.get(\"pad_to_32\", False),\n",
    "                               preserve_label_space=kwargs.get(\"separate_labels_space\")) for cl in classes_lst]\n",
    "    test_loaders = [ds.test_loader for ds in dataset]\n",
    "    train_loaders = [ds.train_loader for ds in dataset]\n",
    "    return train_loaders, test_loaders\n",
    "\n",
    "\n",
    "def ds_padded_split_mnist(**kwargs):\n",
    "    \"\"\"\n",
    "    Split MNIST dataset, padded to 32x32 pixels.\n",
    "    \"\"\"\n",
    "    return ds_split_mnist(pad_to_32=True, **kwargs)\n",
    "\n",
    "def ds_permuted_mnist(**kwargs):\n",
    "    \"\"\"\n",
    "    Permuted MNIST dataset.\n",
    "    First task is the MNIST datasets (with 10 possible labels).\n",
    "    Other tasks are permutations (pixel-wise) of the MNIST datasets (with 10 possible labels).\n",
    "    :param batch_size: batch size\n",
    "           num_workers: num of workers\n",
    "           pad_to_32: If true, will pad digits to size 32x32 and normalize to zero mean and unit variance.\n",
    "           permutations: A list of permutations. Each permutation should be a list containing new pixel position.\n",
    "           separate_labels_space: True for seperated labels space - task i labels will be (10*i) to (10*i + 9).\n",
    "                                  False for unified labels space - all tasks will have labels of 0 to 9.\n",
    "    :return: Tuple with two lists.\n",
    "             First list of the tuple is a list of train loaders, each loader is a task.\n",
    "             Second list of the tuple is a list of test loaders, each loader is a task.\n",
    "    \"\"\"\n",
    "    # First task\n",
    "    dataset = [DatasetsLoaders(\"MNIST\", batch_size=kwargs.get(\"batch_size\", 128),\n",
    "                               num_workers=kwargs.get(\"num_workers\", 1), pad_to_32=kwargs.get(\"pad_to_32\", False))]\n",
    "    target_offset = 0\n",
    "    permutations = kwargs.get(\"permutations\", [])\n",
    "    for pidx in range(len(permutations)):\n",
    "        if kwargs.get(\"separate_labels_space\"):\n",
    "            target_offset = (pidx + 1) * 10\n",
    "        dataset.append(DatasetsLoaders(\"MNIST\", batch_size=kwargs.get(\"batch_size\", 128),\n",
    "                                       num_workers=kwargs.get(\"num_workers\", 1),\n",
    "                                       permutation=permutations[pidx], target_offset=target_offset,\n",
    "                                       pad_to_32=kwargs.get(\"pad_to_32\", False)))\n",
    "    # For offline permuted we take the datasets and mix them.\n",
    "    if kwargs.get(\"offline\", False):\n",
    "        train_sets = []\n",
    "        test_sets = []\n",
    "        for ds in dataset:\n",
    "            train_sets.append(ds.train_set)\n",
    "            test_sets.append(ds.test_set)\n",
    "        train_set = torch.utils.data.ConcatDataset(train_sets)\n",
    "        test_set = torch.utils.data.ConcatDataset(test_sets)\n",
    "        train_loader = torch.utils.data.DataLoader(train_set, batch_size=kwargs.get(\"batch_size\", 128), shuffle=True,\n",
    "                                                   num_workers=kwargs.get(\"num_workers\", 1), pin_memory=True)\n",
    "        test_loader = torch.utils.data.DataLoader(test_set, batch_size=kwargs.get(\"batch_size\", 128), shuffle=False,\n",
    "                                                  num_workers=kwargs.get(\"num_workers\", 1), pin_memory=True)\n",
    "        return [train_loader], [test_loader]\n",
    "    test_loaders = [ds.test_loader for ds in dataset]\n",
    "    train_loaders = [ds.train_loader for ds in dataset]\n",
    "    return train_loaders, test_loaders\n",
    "\n",
    "\n",
    "def ds_padded_permuted_mnist(**kwargs):\n",
    "    \"\"\"\n",
    "    Permuted MNIST dataset, padded to 32x32.\n",
    "    \"\"\"\n",
    "    return ds_permuted_mnist(pad_to_32=True, **kwargs)\n",
    "\n",
    "\n",
    "'''This is an important method which actually sends in all keyword arguments and other stuff that provides \n",
    "us an insight into the parameters that DatasetsLoaders class takes in and operates with.\n",
    "'''\n",
    "def ds_padded_cont_permuted_mnist(**kwargs):\n",
    "    \"\"\"\n",
    "    Continuous Permuted MNIST dataset, padded to 32x32.\n",
    "    Notice that this dataloader is aware to the epoch number, therefore if the training is loaded from a checkpoint\n",
    "        adjustments might be needed. \n",
    "    Access dataset.tasks_probs_over_iterations to see the tasks probabilities for each iteration.\n",
    "    :param num_epochs: Number of epochs for the training (since it builds distribution over iterations,\n",
    "                            it needs this information in advance)\n",
    "    :param iterations_per_virtual_epc: In continuous task-agnostic learning, the notion of epoch does not exists,\n",
    "                                        since we cannot define 'passing over the whole dataset'. Therefore,\n",
    "                                        we define \"iterations_per_virtual_epc\" -\n",
    "                                        how many iterations consist a single epoch.\n",
    "    :param contpermuted_beta: The proportion in which the tasks overlap. 4 means that 1/4 of a task duration will\n",
    "                                consist of data from previous/next task. Larger values means less overlapping.\n",
    "    :param permutations: The permutations which will be used (first task is always the original MNIST).\n",
    "    :param batch_size: Batch size.\n",
    "    :param num_workers: Num workers.\n",
    "    :return: A tuple of (train_loaders, test_loaders). train_loaders is a list of 1 data loader - it loads the\n",
    "                permuted MNIST dataset continuously as described in the paper. test_loaders is a list of 1+permutations\n",
    "                data loaders, one for each dataset.\n",
    "\n",
    "    \"\"\"\n",
    "    dataset = [DatasetsLoaders(\"CONTPERMUTEDPADDEDMNIST\", batch_size=kwargs.get(\"batch_size\", 128),\n",
    "                               num_workers=kwargs.get(\"num_workers\", 1),\n",
    "                               total_iters=(kwargs.get(\"num_epochs\")*kwargs.get(\"iterations_per_virtual_epc\")),\n",
    "                               contpermuted_beta=kwargs.get(\"contpermuted_beta\"),\n",
    "                               iterations_per_virtual_epc=kwargs.get(\"iterations_per_virtual_epc\"),\n",
    "                               all_permutation=kwargs.get(\"permutations\", []))]\n",
    "    test_loaders = [tloader for ds in dataset for tloader in ds.test_loader]\n",
    "    train_loaders = [ds.train_loader for ds in dataset]\n",
    "\n",
    "    return train_loaders, test_loaders\n",
    "\n",
    "#Same as the above method,only thing being padded has been removed and image size would be 28x28 only.\n",
    "def ds_cont_permuted_mnist(**kwargs):\n",
    "    \"\"\"\n",
    "    Continuous Permuted MNIST dataset\n",
    "    Notice that this dataloader is aware to the epoch number, therefore if the training is loaded from a checkpoint\n",
    "        adjustments might be needed. \n",
    "    Access dataset.tasks_probs_over_iterations to see the tasks probabilities for each iteration.\n",
    "    :param num_epochs: Number of epochs for the training (since it builds distribution over iterations,\n",
    "                            it needs this information in advance)\n",
    "    :param iterations_per_virtual_epc: In continuous task-agnostic learning, the notion of epoch does not exists,\n",
    "                                        since we cannot define 'passing over the whole dataset'. Therefore,\n",
    "                                        we define \"iterations_per_virtual_epc\" -\n",
    "                                        how many iterations consist a single epoch.\n",
    "    :param contpermuted_beta: The proportion in which the tasks overlap. 4 means that 1/4 of a task duration will\n",
    "                                consist of data from previous/next task. Larger values means less overlapping.\n",
    "    :param permutations: The permutations which will be used (first task is always the original MNIST).\n",
    "    :param batch_size: Batch size.\n",
    "    :param num_workers: Num workers.\n",
    "    :return: A tuple of (train_loaders, test_loaders). train_loaders is a list of 1 data loader - it loads the\n",
    "                permuted MNIST dataset continuously as described in the paper. test_loaders is a list of 1+permutations\n",
    "                data loaders, one for each dataset.\n",
    "\n",
    "    \"\"\"\n",
    "    dataset = [DatasetsLoaders(\"CONTPERMUTEDMNIST\", batch_size=kwargs.get(\"batch_size\", 128),\n",
    "                               num_workers=kwargs.get(\"num_workers\", 1),\n",
    "                               total_iters=(kwargs.get(\"num_epochs\")*kwargs.get(\"iterations_per_virtual_epc\")),\n",
    "                               contpermuted_beta=kwargs.get(\"contpermuted_beta\"),\n",
    "                               iterations_per_virtual_epc=kwargs.get(\"iterations_per_virtual_epc\"),\n",
    "                               all_permutation=kwargs.get(\"permutations\", []))]\n",
    "    test_loaders = [tloader for ds in dataset for tloader in ds.test_loader]\n",
    "    train_loaders = [ds.train_loader for ds in dataset]\n",
    "\n",
    "    return train_loaders, test_loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_permutation = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 28*28\n",
    "for p_idx in range(9):\n",
    "    permutation = list(range(input_size))\n",
    "    random.shuffle(permutation)\n",
    "    all_permutation.append(permutation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am here\n"
     ]
    }
   ],
   "source": [
    "dataset = DatasetsLoaders(\"CONTPERMUTEDMNIST\", batch_size=128,\n",
    "                               num_workers=1,\n",
    "                               total_iters=100*469,\n",
    "                               contpermuted_beta=4,\n",
    "                               iterations_per_virtual_epc=469,\n",
    "                               all_permutation=all_permutation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = dataset.train_loader\n",
    "test_loaders = dataset.test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_task_probs(iters, tasks, task_id, beta=3):\n",
    "    if beta <= 1:\n",
    "        peak_start = int((task_id/tasks)*iters)\n",
    "        peak_end = int(((task_id + 1) / tasks)*iters)\n",
    "        start = peak_start\n",
    "        end = peak_end\n",
    "    else:\n",
    "        start = max(int(((beta*task_id - 1)*iters)/(beta*tasks)), 0)\n",
    "        peak_start = int(((beta*task_id + 1)*iters)/(beta*tasks))\n",
    "        peak_end = int(((beta * task_id + (beta - 1)) * iters) / (beta * tasks))\n",
    "        end = min(int(((beta * task_id + (beta + 1)) * iters) / (beta * tasks)), iters)\n",
    "\n",
    "    #This is a probability dist for each task across iterations\n",
    "    probs = torch.zeros(iters, dtype=torch.float)\n",
    "\n",
    "    if task_id == 0:\n",
    "        probs[start:peak_start].add_(1)\n",
    "    else:\n",
    "        probs[start:peak_start] = _get_linear_line(start, peak_start, direction=\"up\")\n",
    "    probs[peak_start:peak_end].add_(1)\n",
    "    if task_id == tasks - 1:\n",
    "        probs[peak_end:end].add_(1)\n",
    "    else:\n",
    "        probs[peak_end:end] = _get_linear_line(peak_end, end, direction=\"down\")\n",
    "    \n",
    "    # with open('probs.txt','w') as f:\n",
    "    #     f.write(str(probs.numpy().tolist()))\n",
    "\n",
    "    return probs,end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_tasks = 10\n",
    "beta = 4\n",
    "total_iters = 100*469\n",
    "\n",
    "task_ends = [create_task_probs(total_iters, total_tasks, task_id,beta)[1] for task_id in range(total_tasks)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5862, 10552, 15242, 19932, 24622, 29312, 34002, 38692, 43382, 46900]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_ends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bgd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
